import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, random_split
from data_provider.stock_loader import StockDataset
from models.stock_gpt import StockGPT
from utils.early_stop import EarlyStopping
from utils.metrics import mae, rmse
from config.config import StockConfig
import os
import matplotlib.pyplot as plt


class Exp_Stock:
    def __init__(self, args):
        self.args = args
        self.device = args.device

        # æ•°æ®åŠ è½½
        full_dataset = StockDataset(args.data_path, seq_len=args.seq_len)
        val_size = int(0.2 * len(full_dataset))
        train_size = len(full_dataset) - val_size

        self.train_dataset, self.val_dataset = random_split(full_dataset, [train_size, val_size])
        self.train_loader = DataLoader(self.train_dataset, batch_size=args.batch_size, shuffle=True)
        self.val_loader = DataLoader(self.val_dataset, batch_size=args.batch_size, shuffle=False)

        # æ¨¡å‹é…ç½®
        config = StockConfig(block_size=args.seq_len)
        self.model = StockGPT(
            seq_len=args.seq_len,
            d_model=config.hidden_dim,
            dropout=config.dropout,
        ).to(self.device)

        print(f"æ¨¡å‹ç»“æ„é…ç½®ï¼š{config}")
        print(f"æ¨¡å‹æ€»å‚æ•°é‡: {sum(p.numel() for p in self.model.parameters())}")

        # ä¼˜åŒ–å™¨ä¸æŸå¤±å‡½æ•°
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=args.lr)
        self.criterion = nn.MSELoss()

        # æ—©åœä¸ä¿å­˜è·¯å¾„
        self.early_stopper = EarlyStopping(patience=10, verbose=True)
        self.checkpoint_path = os.path.join(args.checkpoints, "best_model.pth")

    def train_batch(self, X_batch, y_batch):
        self.model.train()
        X_batch, y_batch = X_batch.to(self.device), y_batch.to(self.device)

        preds = self.model(X_batch).squeeze(-1)
        y_batch = y_batch.squeeze(-1)

        loss = self.criterion(preds, y_batch)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        return loss.item()

    def evaluate(self):
        self.model.eval()
        total_loss = 0
        all_preds, all_targets = [], []

        with torch.no_grad():
            for X_batch, y_batch in self.val_loader:
                X_batch, y_batch = X_batch.to(self.device), y_batch.to(self.device)
                preds = self.model(X_batch).squeeze(-1)
                y_batch = y_batch.squeeze(-1)

                loss = self.criterion(preds, y_batch)
                total_loss += loss.item()

                all_preds.append(preds.cpu().numpy())
                all_targets.append(y_batch.cpu().numpy())

        avg_loss = total_loss / len(self.val_loader)
        preds_np = np.concatenate(all_preds)
        targets_np = np.concatenate(all_targets)
        return avg_loss, mae(preds_np, targets_np), rmse(preds_np, targets_np)

    def train(self):
        print(f"å¼€å§‹è®­ç»ƒï¼Œå…± {len(self.train_loader)} ä¸ªè®­ç»ƒæ‰¹æ¬¡ï¼Œ{len(self.val_loader)} ä¸ªéªŒè¯æ‰¹æ¬¡")
        train_losses, val_losses = [], []

        for epoch in range(1, self.args.epochs + 1):
            epoch_train_loss = [self.train_batch(X, y) for X, y in self.train_loader]
            avg_train_loss = np.mean(epoch_train_loss)
            train_losses.append(avg_train_loss)

            val_loss, val_mae, val_rmse = self.evaluate()
            val_losses.append(val_loss)

            print(f"Epoch {epoch}/{self.args.epochs} | "
                  f"Train Loss: {avg_train_loss:.6f} | Val Loss: {val_loss:.6f} | "
                  f"MAE: {val_mae:.4f} | RMSE: {val_rmse:.4f}")

            # ä½¿ç”¨æ—©åœæœºåˆ¶
            self.early_stopper(val_loss, self.model, self.checkpoint_path)

            if self.early_stopper.early_stop:
                print("âš ï¸ Early stopping è§¦å‘ï¼Œè®­ç»ƒç»ˆæ­¢ã€‚")
                break

        # è®­ç»ƒå®Œæˆåç»˜åˆ¶æŸå¤±æ›²çº¿
        self.plot_loss(train_losses, val_losses)

        # æœ€ç»ˆç¡®è®¤æ¨¡å‹ä¿å­˜
        if os.path.exists(self.checkpoint_path):
            print(f"âœ… æœ€ä¼˜æ¨¡å‹å·²ä¿å­˜: {self.checkpoint_path}")
        else:
            print("âš ï¸ æœªæ£€æµ‹åˆ°æ¨¡å‹ä¿å­˜ï¼Œè¯·æ£€æŸ¥ EarlyStopping é€»è¾‘ã€‚")

    def plot_loss(self, train_losses, val_losses):
        plt.figure(figsize=(8, 5))
        plt.plot(train_losses, label="Train Loss", linewidth=2)
        plt.plot(val_losses, label="Val Loss", linewidth=2)
        plt.xlabel("Epoch")
        plt.ylabel("Loss")
        plt.title("Training & Validation Loss")
        plt.legend()
        plt.grid(True, linestyle="--", alpha=0.6)

        save_path = os.path.join(self.args.checkpoints, "loss_curve.png")
        plt.savefig(save_path, dpi=300)
        plt.close()
        print(f"ğŸ“ˆ Loss æ›²çº¿å·²ä¿å­˜åˆ°: {save_path}")
