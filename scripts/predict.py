import os
import sys
import argparse
from datetime import timedelta
import joblib

import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['axes.unicode_minus'] = False
for font in ['Microsoft YaHei', 'SimHei', 'SimSun', 'KaiTi', 'FangSong']:
    if font in [f.name for f in matplotlib.font_manager.fontManager.ttflist]:
        plt.rcParams['font.sans-serif'] = [font]
        break
else:
    plt.rcParams['font.sans-serif'] = ['DejaVu Sans']

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config.config import StockConfig
from models.models import StockGPT
from data_provider.stock_loader import StockDataset
from utils.metrics import mae, rmse

# ---------------------------
# å†å²è¯„ä¼°
# ---------------------------
def evaluate_history(model, dataset, device, output_dir):
    model.eval()
    X_test = torch.tensor(dataset.X, dtype=torch.float32).to(device)
    y_test = dataset.y  # shape=(N,1)

    preds = []
    with torch.no_grad():
        batch_size = 64
        for i in range(0, len(X_test), batch_size):
            batch = X_test[i:i+batch_size]
            pred = model(batch)
            if pred.dim() == 3:
                pred = pred[:, -1, :]
            preds.append(pred.cpu().numpy())

    preds = np.concatenate(preds, axis=0)

    # åå½’ä¸€åŒ–
    preds_denorm = dataset.target_scaler.inverse_transform(preds)
    y_denorm = dataset.target_scaler.inverse_transform(y_test)

    # æ—¥æœŸ
    df = pd.read_csv(dataset.file_path, parse_dates=['æ—¥æœŸ']).sort_values('æ—¥æœŸ')
    dates = df['æ—¥æœŸ'].values[-len(y_denorm):]

    mae_score = mae(preds_denorm, y_denorm)
    rmse_score = rmse(preds_denorm, y_denorm)

    print(f"\nå†å²è¯„ä¼°ç»“æœ: MAE={mae_score:.4f}, RMSE={rmse_score:.4f}")

    plt.figure(figsize=(14,6))
    plt.plot(dates, y_denorm, label="çœŸå®å€¼")
    plt.plot(dates, preds_denorm, label="é¢„æµ‹å€¼")
    plt.title(f"å†å²é¢„æµ‹ (MAE: {mae_score:.4f}, RMSE: {rmse_score:.4f})")
    plt.xlabel("æ—¥æœŸ")
    plt.ylabel("è‚¡ä»·")
    plt.legend()
    plt.grid(True)
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, "prediction_history.png"), dpi=300)
    plt.close()

# ---------------------------
# é¢„æµ‹æœªæ¥ N å¤©
# ---------------------------
# def predict_future(model, dataset, device, seq_len, days):
#     model.eval()
#     df = pd.read_csv(dataset.file_path, parse_dates=['æ—¥æœŸ']).sort_values('æ—¥æœŸ')
#
#     # 4 ä¸ªç‰¹å¾
#     features = df[dataset.feature_cols].values
#
#     # æœ€è¿‘ seq_len è¡Œ
#     last_seq = features[-seq_len:]
#
#     # ä½¿ç”¨åŠ è½½çš„ scaler å½’ä¸€åŒ–
#     norm_seq = dataset.feature_scaler.transform(last_seq)
#
#     current_seq = torch.tensor(norm_seq, dtype=torch.float32).unsqueeze(0).to(device)
#
#     preds = []
#     for _ in range(days):
#         with torch.no_grad():
#             pred = model(current_seq)
#             if pred.dim() == 3:
#                 pred = pred[:, -1, :]
#         next_price = pred.cpu().numpy()[0, 0]
#         preds.append(next_price)
#
#         # å°†é¢„æµ‹è¡¥å…¥åºåˆ—ï¼š4 ç»´ä¸­åªæœ‰â€œæ”¶ç›˜â€è¢«é¢„æµ‹
#         fake_next = np.array([[next_price, last_seq[-1,1], last_seq[-1,2], last_seq[-1,3]]])
#         fake_next_norm = dataset.feature_scaler.transform(fake_next)
#
#         next_input = torch.tensor(fake_next_norm, dtype=torch.float32).unsqueeze(0).to(device)
#         current_seq = torch.cat([current_seq[:, 1:, :], next_input], dim=1)
#
#     future_norm = np.array(preds).reshape(-1,1)
#     return dataset.target_scaler.inverse_transform(future_norm).flatten()

# ---------------------------
# é¢„æµ‹ä¸‹ä¸€å¤©æ”¶ç›˜ä»·ï¼ˆå•æ­¥ï¼‰
# ---------------------------
def predict_future(model, dataset, device, seq_len, days=1):
    assert days == 1, "æœ¬å‡½æ•°ä»…æ”¯æŒé¢„æµ‹ä¸‹ä¸€å¤©"
    model.eval()
    df = pd.read_csv(dataset.file_path, parse_dates=['æ—¥æœŸ']).sort_values('æ—¥æœŸ')

    # 1. å–æœ€å seq_len æ¡åŸå§‹æ•°æ®
    raw = df[dataset.feature_cols].values[-seq_len:].astype(np.float32)
    # æˆäº¤é‡ log1p ä¸è®­ç»ƒä¿æŒä¸€è‡´
    if 'æˆäº¤é‡' in dataset.feature_cols:
        raw[:, dataset.feature_cols.index('æˆäº¤é‡')] = np.log1p(raw[:, dataset.feature_cols.index('æˆäº¤é‡')])

    # 2. å½’ä¸€åŒ–
    norm = dataset.feature_scaler.transform(raw)
    x = torch.tensor(norm, dtype=torch.float32).unsqueeze(0).to(device)

    # 3. å•æ­¥é¢„æµ‹
    with torch.no_grad():
        pred_norm = model(x)
        if pred_norm.dim() == 3:
            pred_norm = pred_norm[:, -1, :]
    next_price = dataset.target_scaler.inverse_transform(pred_norm.cpu().numpy()).item()

    return np.array([next_price])

# ---------------------------
# ä¿å­˜æœªæ¥é¢„æµ‹
# ---------------------------
def save_predictions(prices, dataset, output_dir):
    df = pd.read_csv(dataset.file_path, parse_dates=['æ—¥æœŸ']).sort_values('æ—¥æœŸ')
    last_date = df['æ—¥æœŸ'].iloc[-1]
    future_dates = []
    current = last_date

    while len(future_dates) < len(prices):
        current += timedelta(days=1)
        if current.weekday() < 5:  # å·¥ä½œæ—¥
            future_dates.append(current)

    result = pd.DataFrame({'æ—¥æœŸ': future_dates, 'é¢„æµ‹æ”¶ç›˜ä»·': prices})
    csv_path = os.path.join(output_dir, "future_predictions.csv")
    result.to_csv(csv_path, index=False, encoding="utf-8-sig")
    print(f"æœªæ¥é¢„æµ‹ç»“æœå·²ä¿å­˜: {csv_path}")

    return result

# ---------------------------
# å¯è§†åŒ–æœªæ¥é¢„æµ‹
# ---------------------------
def visualize_predictions(prices, dataset, output_dir):
    df = pd.read_csv(dataset.file_path, parse_dates=['æ—¥æœŸ']).sort_values('æ—¥æœŸ')

    # åªå–æœ€è¿‘ 30 ä¸ªäº¤æ˜“æ—¥
    last_30 = df.tail(30)

    last_date = df['æ—¥æœŸ'].iloc[-1]

    # æ„é€ æœªæ¥æ—¥æœŸ
    future_dates = []
    current = last_date
    while len(future_dates) < len(prices):
        current += timedelta(days=1)
        if current.weekday() < 5:
            future_dates.append(current)

    # ----------- ç»˜å›¾ ----------- #
    plt.figure(figsize=(14,6))

    # æœ€è¿‘ 30 å¤©å†å²
    plt.plot(last_30['æ—¥æœŸ'], last_30['æ”¶ç›˜'], label="å†å²è‚¡ä»·", linewidth=2)

    # æœªæ¥é¢„æµ‹
    plt.plot(future_dates, prices, label="æœªæ¥é¢„æµ‹", linestyle='--')

    # å‚ç›´åˆ†å‰²çº¿ï¼ˆå†å² / æœªæ¥ï¼‰
    plt.axvline(x=last_date, color='gray', linestyle=':')

    # åœ¨æœªæ¥æ›²çº¿ä¸Šæ ‡æ•°å€¼
    for d, p in zip(future_dates, prices):
        plt.text(d, p, f"{p:.2f}", fontsize=10, ha='center', va='bottom')

    plt.xlabel("æ—¥æœŸ")
    plt.ylabel("è‚¡ä»·")
    plt.title("æœªæ¥è‚¡ä»·é¢„æµ‹ï¼ˆæœ€è¿‘ 30 å¤©ï¼‰")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()

    plt.savefig(os.path.join(output_dir,"future_predictions.png"), dpi=300)
    plt.close()

def visualize_truth_with_prediction(truth_path, pred_path, output_dir):
    """
    truth_path: 600519_data_truth.csv
    pred_path: future_predictions.csv
    """

    # --- è¯»å–çœŸå®æ•°æ® ---
    truth_df = pd.read_csv(truth_path, parse_dates=['æ—¥æœŸ'])
    truth_df = truth_df.sort_values('æ—¥æœŸ')

    # --- è¯»å–é¢„æµ‹ç»“æœ ---
    pred_df = pd.read_csv(pred_path, parse_dates=['æ—¥æœŸ'])
    pred_date = pred_df['æ—¥æœŸ'].iloc[0]
    pred_price = pred_df['é¢„æµ‹æ”¶ç›˜ä»·'].iloc[0]

    # --- æå–é¢„æµ‹æ—¥å‰ 30 å¤©ï¼ˆå«å½“å¤©ï¼‰ ---
    mask = (truth_df['æ—¥æœŸ'] <= pred_date)
    last_30 = truth_df.loc[mask].tail(30)

    # æœ€åä¸€å¤©çœŸå®å€¼ï¼ˆç”¨äºæ ‡æ³¨ï¼‰
    last_date = last_30['æ—¥æœŸ'].iloc[-1]
    last_price = last_30['æ”¶ç›˜'].iloc[-1]

    # --- ç»˜å›¾ ---
    plt.figure(figsize=(14,6))

    # å†å² 30 å¤©çœŸå®æ”¶ç›˜ä»·
    plt.plot(last_30['æ—¥æœŸ'], last_30['æ”¶ç›˜'], label="çœŸå®æ”¶ç›˜ä»·ï¼ˆè¿‘30å¤©ï¼‰", linewidth=2)

    # ğŸ”µ æ ‡æ³¨çœŸå®å€¼æœ€åä¸€å¤©
    plt.scatter(last_date, last_price, color='blue', s=70)
    plt.text(last_date, last_price,
             f"{last_price:.2f}",
             fontsize=12, ha='right', va='bottom',
             color='blue')

    # é¢„æµ‹ç‚¹ï¼ˆçº¢è‰²æ ‡è®°ï¼‰
    plt.scatter(pred_date, pred_price, color='red', s=80, label="é¢„æµ‹å€¼")
    plt.text(pred_date, pred_price, f"{pred_price:.2f}",
             fontsize=12, ha='left', va='bottom', color='red')

    plt.title("çœŸå®æ”¶ç›˜ä»·ï¼ˆè¿‘30å¤©ï¼‰ä¸é¢„æµ‹å€¼å¯¹æ¯”")
    plt.xlabel("æ—¥æœŸ")
    plt.ylabel("æ”¶ç›˜ä»·")
    plt.grid(True)
    plt.legend()
    plt.tight_layout()

    save_path = os.path.join(output_dir, "truth_vs_prediction.png")
    plt.savefig(save_path, dpi=300)
    plt.close()

    print(f"å¯¹æ¯”å›¾å·²ä¿å­˜ï¼š{save_path}")

# ---------------------------
# ä¸»å‡½æ•°
# ---------------------------
if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--days', type=int, default=1)
    args = parser.parse_args()

    device = "cuda" if torch.cuda.is_available() else "cpu"
    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

    data_path = os.path.join(base_dir, "scripts", "600519_data.csv")
    checkpoint_path = os.path.join(base_dir, "checkpoints", "best_model.pth")
    feature_scaler_path = os.path.join(base_dir, "checkpoints", "feature_scaler.pkl")
    target_scaler_path  = os.path.join(base_dir, "checkpoints", "target_scaler.pkl")
    output_dir = os.path.join(base_dir, "results")
    os.makedirs(output_dir, exist_ok=True)

    # ---------------------------
    # åŠ è½½ scaler
    # ---------------------------
    feature_scaler, target_scaler = StockDataset.load_scalers(feature_scaler_path, target_scaler_path)
    print("âœ… scalers åŠ è½½å®Œæˆ")

    # ---------------------------
    # åŠ è½½æ¨¡å‹
    # ---------------------------
    cfg = StockConfig()
    model = StockGPT(cfg).to(device)

    checkpoint = torch.load(checkpoint_path, map_location=device)
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
    else:
        model.load_state_dict(checkpoint)
    model.eval()
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

    # ---------------------------
    # åŠ è½½æ•°æ®é›†å¹¶æ›¿æ¢ scalers
    # ---------------------------
    dataset = StockDataset(data_path, seq_len=cfg.seq_len)
    dataset.feature_scaler = feature_scaler
    dataset.target_scaler  = target_scaler

    # å†å²é¢„æµ‹
    evaluate_history(model, dataset, device, output_dir)

    # æœªæ¥é¢„æµ‹
    future_prices = predict_future(model, dataset, device, cfg.seq_len, args.days)
    save_predictions(future_prices, dataset, output_dir)
    # visualize_predictions(future_prices, dataset, output_dir)

    # æœªæ¥é¢„æµ‹æ–‡ä»¶è·¯å¾„
    pred_csv = os.path.join(output_dir, "future_predictions.csv")
    # çœŸå®æ•°æ®è·¯å¾„
    truth_csv = os.path.join(base_dir, "scripts", "600519_data_truth.csv")
    # ç»˜åˆ¶çœŸå® 30 å¤©ä¸é¢„æµ‹ç‚¹å¯¹æ¯”
    visualize_truth_with_prediction(truth_csv, pred_csv, output_dir)

    print("\nâœ… é¢„æµ‹ä»»åŠ¡å®Œæˆï¼")
